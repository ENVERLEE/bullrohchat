# crawler.py
import re
import time
import json
import requests # Added for direct API calls
from typing import Dict, Any, List, Optional
from tqdm import tqdm # Add this import
from dotenv import load_dotenv
# Import necessary libraries from Selenium for web scraping
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Import BeautifulSoup for parsing HTML content
from bs4 import BeautifulSoup

load_dotenv()

class BlogCrawler:
    """
    A class to crawl all posts from a Naver blog.
    This version is enhanced to use Naver's internal API for fetching post lists,
    and has refined selectors for accurately scraping post content.
    """
    # Define the base URL for Naver Blog PC version.
    BASE_URL_PC = "https://blog.naver.com"

    # [Refined] Define robust CSS selectors for post content details
    CONTENT_SELECTORS = ".se-main-container, .post_content, .se-component.se-text.se-section, .sect_dsc, .post_ct, #content-area .post_content, .se-module-text, .pcol1 .post_content, .se-main-container, .post-view"
    WRITER_SELECTORS = ".nick_name, .blog_author .author_name, .author, .writer, .nickname, .blog_name, .blog_name, .nickname"
    DATE_SELECTORS = ".se_time, .blog_header_info .date, ._postContents .post_info .date, .post_date, .date, .write_date, .se_publishDate, .date"
    HASHTAG_SELECTORS = "div.post_tag a, .se_component_hashtag a"


    def __init__(self, headless: bool = False):
        """
        Initializes the Selenium WebDriver.

        Args:
            headless (bool): If True, runs the browser in headless mode (without a GUI).
        """
        # Set up options for the Chrome WebDriver.
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36")
        
        try:
            # Initialize the Chrome WebDriver for scraping post details.
            self.driver = webdriver.Chrome(options=options)
            print("âœ… WebDriver has been successfully initialized.")
        except Exception as e:
            # Handle exceptions during WebDriver setup.
            print(f"âŒ WebDriver setup failed: {e}")
            print("  - Please ensure that ChromeDriver is installed and its path is registered in your system's PATH.")
            raise

    def _extract_blog_id(self, blog_url: str) -> Optional[str]:
        """
        Extracts the blog ID from a given Naver Blog URL.
        """
        # This regex is robust for various Naver blog URL formats.
        match = re.search(r'blog\.naver\.com/([a-zA-Z0-9_-]+)', blog_url)
        if match:
            return match.group(1)
        print(f"âŒ Could not extract a valid blog ID from the URL: {blog_url}")
        return None

    def get_image_src(self, img_element):
        """
        ì´ë¯¸ì§€ ìš”ì†Œì—ì„œ ìœ íš¨í•œ src URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        srcì— 'blur'ê°€ í¬í•¨ëœ ê²½ìš° data-lazy-srcë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        src = img_element.get_attribute('src')
        lazy_src = img_element.get_attribute('data-lazy-src')

        is_blur = False
        if src and 'blur' in src.lower(): # srcì— 'blur' ë¬¸ìì—´ì´ ìˆëŠ”ì§€ í™•ì¸ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
            is_blur = True
            print(f"    - Blur ì´ë¯¸ì§€ ê°ì§€ë¨: {src[:60]}...") # ë¡œê·¸ ì¶”ê°€

        is_lazy_valid = lazy_src and lazy_src.startswith('http')
        is_src_valid = src and src.startswith('http') and not src.startswith('data:image')

        if is_blur:
            # Blur ì´ë¯¸ì§€ì¼ ê²½ìš°, ìœ íš¨í•œ lazy_srcê°€ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
            if is_lazy_valid:
                print(f"    - data-lazy-src ì‚¬ìš©: {lazy_src[:60]}...") # ë¡œê·¸ ì¶”ê°€
                return lazy_src
            else:
                # Blur ì´ë¯¸ì§€ì¸ë° ìœ íš¨í•œ lazy_srcê°€ ì—†ìœ¼ë©´, í•´ë‹¹ ì´ë¯¸ì§€ëŠ” ê°€ì ¸ì˜¤ì§€ ì•ŠìŒ (None ë°˜í™˜)
                print(f"    - Blur ì´ë¯¸ì§€ì§€ë§Œ ìœ íš¨í•œ data-lazy-src ì—†ìŒ.") # ë¡œê·¸ ì¶”ê°€
                return None
        elif is_src_valid:
            # Blur ì´ë¯¸ì§€ê°€ ì•„ë‹ˆê³  srcê°€ ìœ íš¨í•˜ë©´ srcë¥¼ ì‚¬ìš©
            return src
        elif is_lazy_valid:
            # srcê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì—†ì§€ë§Œ lazy_srcê°€ ìœ íš¨í•˜ë©´ lazy_src ì‚¬ìš©
            # (srcê°€ blurê°€ ì•„ë‹ˆë©´ì„œ data URI ì´ê±°ë‚˜ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš° ë“±)
            print(f"    - src ë¶€ì í•©, data-lazy-src ì‚¬ìš©: {lazy_src[:60]}...") # ë¡œê·¸ ì¶”ê°€
            return lazy_src
        else:
            # ì–´ë–¤ ìœ íš¨í•œ URLë„ ì°¾ì§€ ëª»í•¨
            return None

    def crawl_all_posts(self, blog_url: str, max_posts: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        [Optimized] Retrieves the URL and title of all posts using Naver's internal API.
        This method is now much faster as it no longer uses Selenium for listing posts.

        Args:
            blog_url (str): The URL of the Naver blog.

        Returns:
            List[Dict[str, Any]]: A list of dictionaries, each containing the 'url' and 'title' of a post.
        """
        blog_id = self._extract_blog_id(blog_url)
        if not blog_id:
            return []

        print(f"\nâ–¶ï¸ Collecting all post listings via API... (ID: {blog_id})")
        all_posts_meta = []
        page = 1

        while True:
            # The API endpoint discovered from the Go code. We fetch 30 items per page for efficiency.
            api_url = f"https://blog.naver.com/PostTitleListAsync.naver?blogId={blog_id}&currentPage={page}&countPerPage=30"
            
            try:
                # Make a direct HTTP request.
                response = requests.get(api_url, timeout=10)
                response.raise_for_status()

                # The API returns JSON-like text with single quotes, which is invalid JSON.
                # We replace them with double quotes to parse correctly.
                response_text = response.text.replace("'", '"')
                data = json.loads(response_text)

                post_list = data.get("postList")
                if not post_list:
                    # If postList is empty, we've reached the last page.
                    print(f"  - No more posts found on page {page}. Finishing collection.")
                    break
                
                for post in post_list:
                    # Ensure 'post' is a dictionary before trying to access its keys
                    if not isinstance(post, dict):
                        print(f"  - âš ï¸ Skipped non-dictionary item in post list: {post}")
                        continue

                    log_no = post.get("logNo")
                    if log_no:
                        # Construct the full, permanent post URL.
                        post_url = f"{self.BASE_URL_PC}/{blog_id}/{log_no}"
                        all_posts_meta.append({
                            'url': post_url,
                            'title': str(post.get("title", "")).strip() # Ensure title is string before strip
                        })
                        
                        # Check if max_posts limit is reached
                        if max_posts is not None and len(all_posts_meta) >= max_posts:
                            print(f"  - Reached maximum post limit of {max_posts}. Stopping collection.")
                            break # Break from the for loop

                print(f"  - Page {page}: collected {len(post_list)} post metadata items.")
                page += 1
                time.sleep(0.5) # Be respectful to the server

                # If max_posts limit was reached in the inner loop, break from the outer loop too
                if max_posts is not None and len(all_posts_meta) >= max_posts:
                    break

            except requests.RequestException as e:
                print(f"  - âŒ API request failed on page {page}: {e}")
                break
            except json.JSONDecodeError:
                print(f"  - âŒ Failed to parse API response on page {page}.")
                break
        
        print(f"âœ… Collected metadata for a total of {len(all_posts_meta)} posts via API.")
        
        # Trim the list to max_posts if it exceeded the limit in the last page
        if max_posts is not None:
            return all_posts_meta[:max_posts]
        return all_posts_meta

    def crawl_post_content(self, post_url: str) -> Dict[str, Any]:
        """
        [UPDATED] Extracts detailed information from a single post URL.
        Uses the new text extraction logic from the markdown crawler.
        
        Args:
            post_url (str): The URL of the blog post.

        Returns:
            Dict[str, Any]: A dictionary containing the post's content, hashtags, writer, and date.
        """
        markdown_output = []  # ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        
        try:
            print(f"ê²Œì‹œë¬¼ ì ‘ì† ì‹œë„: {post_url}")
            self.driver.get(post_url)
            # í˜ì´ì§€ ë¡œë”© ë° JS ì‹¤í–‰ ì¶©ë¶„íˆ ëŒ€ê¸°
            print("í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
            time.sleep(5)

            # --- iframeìœ¼ë¡œ ì „í™˜ ---
            try:
                print("iframeìœ¼ë¡œ ì „í™˜ ì‹œë„...")
                WebDriverWait(self.driver, 15).until(
                    EC.frame_to_be_available_and_switch_to_it((By.ID, "mainFrame"))
                )
                print("iframeìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.")
                time.sleep(1)

                # --- ì œëª© ì°¾ê¸° ---
                title = "ì œëª© ì—†ìŒ"
                try:
                    title_selectors = [
                        '.se-title-text', '.title_text', '.se_title > .se_textView > .se_textarea', '#title_1 > span'
                    ]
                    for selector in title_selectors:
                        try:
                            title_element = WebDriverWait(self.driver, 5).until(
                                EC.visibility_of_element_located((By.CSS_SELECTOR, selector))
                            )
                            if title_element:
                                title = title_element.text.strip()
                                print(f"ì œëª© ì°¾ìŒ: {title}")
                                break
                        except (TimeoutException, NoSuchElementException):
                            continue
                    if title == "ì œëª© ì—†ìŒ":
                        print("âš  ì œëª© ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

                except Exception as e:
                    print(f"âš  ì œëª© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    title = "ì œëª© ì¶”ì¶œ ì˜¤ë¥˜"

                # --- ë³¸ë¬¸ ë‚´ìš© ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ ---
                try:
                    print("ë³¸ë¬¸ ì»¨í…ì¸  ìš”ì†Œ ë¶„ì„ ì‹œì‘...")
                    # ìŠ¤ë§ˆíŠ¸ì—ë””í„°ì˜ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
                    content_container = WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, '.se-main-container'))
                    )
                    print("ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì°¾ìŒ (.se-main-container)")

                    # ì»¨í…Œì´ë„ˆ ë‚´ì˜ ëª¨ë“  'se-component' ìš”ì†Œë“¤ì„ ìˆœì„œëŒ€ë¡œ ê°€ì ¸ì˜´
                    components = content_container.find_elements(By.CSS_SELECTOR, '.se-component')
                    print(f"ì´ {len(components)}ê°œì˜ ì»´í¬ë„ŒíŠ¸(ë¸”ë¡) ë°œê²¬.")

                    if not components:  # êµ¬ë²„ì „ ì—ë””í„° ë“± ë‹¤ë¥¸ êµ¬ì¡°ì¼ ê²½ìš° ëŒ€ë¹„
                        print(".se-component ì—†ìŒ. ëŒ€ì²´ ì„ íƒì ì‹œë„ (#postViewArea)...")
                        content_container = WebDriverWait(self.driver, 5).until(
                            EC.presence_of_element_located((By.ID, 'postViewArea'))
                        )
                        # êµ¬ë²„ì „ ì—ë””í„°ëŠ” ìì‹ ìš”ì†Œ êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ - ë‹¨ìˆœí™”ëœ ì ‘ê·¼
                        all_children = content_container.find_elements(By.XPATH, "./*")  # ëª¨ë“  ì§ê³„ ìì‹
                        print(f"#postViewArea ë‚´ ì§ê³„ ìì‹ {len(all_children)}ê°œ ë°œê²¬.")
                        components = all_children  # ìì‹ ìš”ì†Œë“¤ì„ ì»´í¬ë„ŒíŠ¸ë¡œ ê°„ì£¼

                    if not components:
                        print("âš  ë¶„ì„í•  ë³¸ë¬¸ ì»´í¬ë„ŒíŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

                    processed_components = 0
                    for index, component in enumerate(components):
                        component_processed = False
                        # 1. í…ìŠ¤íŠ¸ ì»´í¬ë„ŒíŠ¸ì¸ì§€ í™•ì¸ (ë‚´ë¶€ì— í…ìŠ¤íŠ¸ ìš”ì†Œ ì°¾ê¸°)
                        try:
                            # .se-text-paragraph í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ëª¨ë“  í•˜ìœ„ p íƒœê·¸ ì°¾ê¸°
                            text_paragraphs = component.find_elements(By.CSS_SELECTOR, '.se-text-paragraph')
                            if text_paragraphs:
                                component_text = []
                                for p in text_paragraphs:
                                    paragraph_text = p.text.strip()
                                    if paragraph_text:  # ë¹ˆ ì¤„ì€ ì œì™¸
                                        component_text.append(paragraph_text)

                                if component_text:  # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´
                                    markdown_output.append("\n".join(component_text))
                                    print(f"  [{index+1}] í…ìŠ¤íŠ¸ ë¸”ë¡ ì²˜ë¦¬ë¨: '{component_text[0][:30]}...'")
                                    component_processed = True

                        except NoSuchElementException:
                            pass  # í…ìŠ¤íŠ¸ ìš”ì†Œ ì—†ìœ¼ë©´ ë‹¤ìŒ í™•ì¸ìœ¼ë¡œ

                        # 2. ì´ë¯¸ì§€ ì»´í¬ë„ŒíŠ¸ì¸ì§€ í™•ì¸ (ë‚´ë¶€ì— ì´ë¯¸ì§€ ìš”ì†Œ ì°¾ê¸°)
                        if not component_processed:  # í…ìŠ¤íŠ¸ê°€ ì•„ë‹ˆì—ˆë‹¤ë©´ ì´ë¯¸ì§€ í™•ì¸
                            try:
                                images = component.find_elements(By.TAG_NAME, 'img')
                                if images:
                                    for img in images:
                                        img_src = self.get_image_src(img)  # ì •ì˜ëœ í•¨ìˆ˜ ì‚¬ìš©
                                        if img_src:
                                            # ì´ë¯¸ì§€ URLì„ ë§ˆí¬ë‹¤ìš´ ì¶œë ¥ì— ì¶”ê°€ (ì‹¤ì œ ë‹¤ìš´ë¡œë“œëŠ” í•˜ì§€ ì•ŠìŒ)
                                            # markdown_output.append(f"[IMAGE: {img_src}]")
                                            print(f"  [{index+1}] ì´ë¯¸ì§€ ë¸”ë¡ ì²˜ë¦¬ë¨: {img_src[:60]}...")
                                            component_processed = True

                            except NoSuchElementException:
                                pass  # ì´ë¯¸ì§€ ìš”ì†Œ ì—†ìœ¼ë©´ ë‹¤ìŒ í™•ì¸ìœ¼ë¡œ

                        # 3. ê¸°íƒ€ ì»´í¬ë„ŒíŠ¸ (ìŠ¤í‹°ì»¤ ë“±) - í˜„ì¬ëŠ” ë¬´ì‹œ
                        if not component_processed:
                            print(f"  [{index+1}] ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì»´í¬ë„ŒíŠ¸ (í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ì•„ë‹˜)")
                            pass  # í•„ìš”ì‹œ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì»´í¬ë„ŒíŠ¸ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€

                        processed_components += 1

                    print(f"ì´ {processed_components}ê°œ ì»´í¬ë„ŒíŠ¸ ë¶„ì„ ì™„ë£Œ.")

                except (TimeoutException, NoSuchElementException):
                    print("âš  ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ(.se-main-container ë˜ëŠ” #postViewArea)ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    print(f"âš  ë³¸ë¬¸ ë‚´ìš© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

                # BeautifulSoupìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                
                # Extract hashtags with improved selectors.
                hashtags = [tag.text.strip().lstrip('#') for tag in soup.select(self.HASHTAG_SELECTORS)]
                
                # Extract additional metadata with improved selectors.
                writer_element = soup.select_one(self.WRITER_SELECTORS)
                date_element = soup.select_one(self.DATE_SELECTORS)
                writer = writer_element.text.strip() if writer_element else "Unknown"
                write_date = date_element.text.strip() if date_element else "Unknown"

                # iframeì—ì„œ ë¹ ì ¸ë‚˜ì˜¤ê¸°
                self.driver.switch_to.default_content()
                print("ê¸°ë³¸ ì»¨í…ì¸ ë¡œ ëŒì•„ì™”ìŠµë‹ˆë‹¤.")

            except TimeoutException:
                print("âš  mainFrame iframeì„ ì°¾ê±°ë‚˜ ì „í™˜í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                markdown_output.append("[ì˜¤ë¥˜: iframeì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.]")
                title = "iframe ì˜¤ë¥˜"
                hashtags = []
                writer = "Unknown"
                write_date = "Unknown"
            except Exception as e:
                print(f"âš  iframe ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                markdown_output.append(f"[ì˜¤ë¥˜: iframe ì²˜ë¦¬ ì¤‘ ë¬¸ì œ ë°œìƒ - {e}]")
                title = "ì²˜ë¦¬ ì˜¤ë¥˜"
                hashtags = []
                writer = "Unknown"
                write_date = "Unknown"

        except TimeoutException:
            print("âš  í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼.")
            return {}
        except Exception as e:
            print(f"âš  í¬ë¡¤ë§ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}

        # ìµœì¢… ì»¨í…ì¸  ìƒì„±
        final_content = "\n\n".join(filter(None, markdown_output))
        
        if not final_content.strip():
            print("âš  ìœ íš¨í•œ ì»¨í…ì¸ ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            final_content = "[ì»¨í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨]"
        
        # ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ìš”ì²­ ì‚¬ì´ì— ì§€ì—° ì¶”ê°€
        time.sleep(1) 

        return {
            'title': title,
            'content': final_content,
            'hashtags': hashtags,
            'writer': writer,
            'write_date': write_date,
        }

    def close(self):
        """
        Closes the WebDriver and releases all associated resources.
        """
        if self.driver:
            self.driver.quit()
            print("âœ… WebDriver has been closed.")


if __name__ == "__main__":
    # The URL of the blog to be crawled.
    BLOG_URL = "https://blog.naver.com/kokos012"
    crawler = BlogCrawler(headless=True)
    MAX_POSTS_TO_CRAWL = 10 # í¬ë¡¤ë§í•  ìµœëŒ€ í¬ìŠ¤íŠ¸ ê°œìˆ˜ ì§€ì •

    try:
        # Step 1: Crawl all post URLs and titles using the fast API method.
        all_posts = crawler.crawl_all_posts(BLOG_URL, max_posts=MAX_POSTS_TO_CRAWL)

        if all_posts:
            # Step 2: Crawl the content of the first N posts as a sample.
            print(f"\n--- Crawling content of {len(all_posts)} posts ---")
            for i, post_meta in enumerate(all_posts):
                print(f"\n[{i+1}/{len(all_posts)}] Crawling \"{post_meta['title']}\"...")
                
                # Get the rich content data for the post.
                content_data = crawler.crawl_post_content(post_meta['url'])
                
                if content_data:
                    # Print the enhanced results.
                    print(f"  - Title: {content_data.get('title')}")
                    print(f"  - Writer: {content_data.get('writer')}")
                    print(f"  - Date: {content_data.get('write_date')}")
                    print(f"  - Hashtags: {content_data.get('hashtags')}")
                    content_preview = content_data.get('content', '')[:200].replace(chr(8203), '') # Show more preview
                    print(f"  - Content (first 200 chars): {content_preview}...")
        else:
            print("\nâš ï¸ No posts were found to crawl.")

    except Exception as e:
        # Catch any exceptions that occur during the main execution.
        print(f"ğŸ”´ An error occurred in the main program: {e}")
    finally:
        # Ensure the WebDriver is always closed.
        crawler.close()
